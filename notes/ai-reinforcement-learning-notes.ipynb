{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Notes\n",
    "\n",
    "**Value based methods** estimate $Q(s,a;\\theta)$. For example Q-learning iteratively minimize:\n",
    "$$L_i(\\theta_i) = \\mathbb{E}[r+\\gamma \\max_{a'}Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i)]$$\n",
    "The n-step version of Q-learning does updates using:\n",
    "$$r_t+\\gamma r_{t+1} + ... + \\gamma^{n-1}r_{t+n-1}+\\gamma^{n}\\max_{a}Q(s_{t+n},a)$$\n",
    "\n",
    "**Policy based methods** parametrize policy $\\pi(a|s;\\theta)$ and update $\\theta$ based on gradient of $\\mathbb{E}[R_t]$. For example the REINFORCE algorithm updates $\\theta$ in the direction of:\n",
    "$$\\nabla_{\\theta}\\log(\\pi(a_t|s_t;\\theta))R_t$$\n",
    "which is an unbiased estimate of $\\nabla_{\\theta}\\mathbb{E}[R_t]$. You can reduce variance while keeping it unbiased by substracting a baseline:\n",
    "$$\\nabla_{\\theta}\\log(\\pi(a_t|s_t;\\theta))(R_t-b(s_t))$$\n",
    "which commonly is: $b(s_t)\\approx V^{\\pi}(s_t)$. The difference $R_t-b_t$ can be seen as an estimate of advantage of using $a_t$  in state $s_t$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:openai]",
   "language": "python",
   "name": "conda-env-openai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
