{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Some notes from Chris lecture\n",
    "\n",
    "- NN - Neural Network\n",
    "- NNs are naturally suitable for adding, but worse for multiplication. We want to be able to multiply because of the fact that it's the common operation you do on beliefs when confronted with evidence (based on Bayes, when you see some data, you update your priors by multiplying some odds with some other odds).\n",
    "- Using logarithms and exponentiation changes multiplication into addition ($xy = e^{\\log(x) + \\log(y)}$).\n",
    "- NNs input is not usually transformed through logarithms, but care is taken to be sure it's reasonable (not too big and not too small), for example it's normalized if necessary ( $(X - EX)/SD(X)$ kind of stuff)\n",
    "- Exponentiated output and divided by sum is what softmax is all about.\n",
    "- Linear combinations of inputs within neurons can be see as operations on information / processing evidence.\n",
    "- Activation functions evolved:\n",
    "    - non continous 0-1 activation - cannot do backpropagation,\n",
    "    - sigmoid - good because we can differentiate it a lot,\n",
    "    - tanh - even better then sigmoid because it produces negative values (it is almost equivalent to scaled and translated sigmoid)\n",
    "    - rectified linear unit (ReLU) - very recent, faster to compute.\n",
    "- the first layer of NN extracts information from input, lots of neurons here may help with classifing more complex patterns - with enough neurons we can approximate any potential transformation of the inputs that we might be tempted to use.\n",
    "- greater amount of hidden layers increases the complexity of possible distributions over priors, \n",
    "- we want to design NN in such a way that it's is exactly enough to discover the level of complexity that we are interested in (anything more thant that slows learning a lot).\n",
    "\n",
    "To read:\n",
    "- https://en.wikipedia.org/wiki/Backpropagation\n",
    "- https://distill.pub/2017/momentum/ (added to 'To Read' in article summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google udacity course on DNN\n",
    "\n",
    "* While training a model, a change in validation set of at least 30 cases (on 30 points on which we used to give wrong label) tends to be statistically significant. That is why in practice validation sets of size of at least 30000 examples are used (to get results significant to first decimal place). This is not true for problems where some classes are very small.\n",
    "* **Stochastic gradient descent**: using small random sample from training set on which to compute loss and gradient update. This is generally worse then using whole dataset (on single iterations it may even increase the loss), but scales well and is fast (you still need to order of magnitude more iterations, but on very small samples). Randomness of samples is essential.\n",
    "    * we want imputs to be of 0 mean and equal small variance,\n",
    "    * we want inital weights to be random, with expected value 0 and equal small variance,\n",
    "    * **momentum**: simple version is using running average of gradient updates instead of the update itself\n",
    "    * learning rate decay - we want to do it, although there is no consensu on how to do it best yet.\n",
    "    * for SGD if your model doesn't perform well a good first thing to try is lowering learning rate (also it is a common thing for models with higher learning rate to initially learn faster, but to reach worse plateau)\n",
    "* **Regularization**\n",
    "    * one way to prevent overfitting would be to observe accuracy on validation set while learning and stop learning when the performance starts to drop\n",
    "    * another approach is to add an additional term to the loss function - usually it would be L1 or L2 norm of the parameters\n",
    "    * some recent approach is called **dropout** - the rough idea is to randomly drop half of units (neurons) from the network for each training example, next, when testing, we use whole network but with weights multiplied by probabilities (so we get expected output from initial network).\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
