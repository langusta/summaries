{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, an introduction, Sutton and Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My open questions:\n",
    "* find example of MDP where optimal eps-greedy policy has value faunction such that the grredy policy with respect to those values is not optimal (in the general sense).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. Dynamic Programming\n",
    "\n",
    "**Key idea of DP:** Use value functions to organize search for good policies.\n",
    "\n",
    "**Key characteristics:**\n",
    "* complete knowledge of environment - knowing the transition probabilities $p(s',r|s,a)$\n",
    "\n",
    "Observations:\n",
    "* **Iterative policy evaluation:** We can use Bellman equations to compute value function for given policy $\\pi$: \n",
    "$$v_{k+1}(s)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma v_k(S_{t+1})| S_t=s] = \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
    "* once we have a value function we can construct new policy that is greedy with respect to the value function:\n",
    "    * if resulting policy is the same as the initial one - great! We have found the optimal policy.\n",
    "    * if it's different then lets find its value function (it's certainly not worse because of policy improvement theorem - see below).\n",
    "* **Policy improvement theorem:**\n",
    "$$\\forall_s q_{\\pi}(s, \\pi'(s)) \\geq v_{\\pi}(s) \\Rightarrow \\forall_s v_{\\pi'}(s)\\geq v_{\\pi}(s)$$\n",
    "* **Policy iteration** is the procedure described above.\n",
    "* It is often not necessary for the value function to converge for the greedy policy with respect to it to be better then the original one. Based on this observation is **value iteration**:\n",
    "$$v_{k+1}(s)= \\max_a\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
    "After it converges, the policy that is greedy with respect to it will be optimal.\n",
    "* it is not necessary to update values of all states in one sweep. In fact even if we update some states more often then the others, as long as all states get updates infinitely many times in the limit, the algorithm will converge. Such procedure is called **asynchronous dynamic programming**.\n",
    "* **Generalized policy iteration** is an idea of letting policy evaluation (finding its value function) and policy improvement (finding better policy based on value function for current policy) interact.\n",
    "* DP methods are polynomial in the number of states and actions.\n",
    "* DP methods assume complete knowledge of the MDP.\n",
    "* **In place updates** are ones in which only one set/table of values is used to do updates. This means that depending on the order some values will be updated using already new other values updated just before.\n",
    "* **Bootstrapping** - an idea of updating estimates based on other estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. Monte Carlo Methods\n",
    "\n",
    "Monte Carlo methods are based only on experience. They don't require a model and do not bootstrap. If you have a model then you can simulate experience. The basic idea is (for episodic tasks): simulate episodes and estimate state-action values as means of observed returns.\n",
    "\n",
    "Observations:\n",
    "* with no model of the environment, given a state, we don't know which actions will lead us to which further states, so what we estimate will be state-action values (not state values)! (Based on values we still wouldn't know which action to choose.)\n",
    "* **soft policy** is a policy that selects all actions in all states with non-zero probability.\n",
    "* **off-policy learning** is the kind of learning where policy used to generate episodes (**behaviour policy**) is defferent then the evaluated and improved one (**target policy**).\n",
    "* when we do off-policy learning then we need to make adjustments to state-action values we are learning based on the fact that behaviour policy chooses action with different frequencies then target policy. Those adjusments are called **importance sampling**. There two basic (and other more complex ones) ways of doing it:\n",
    "\n",
    "    path weights: $$\\rho_t^T=\\frac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\\prod_{k=t}^{T-1}\\mu(A_k|S_k)p(S_{k+1}|S_k, A_k)}=\\prod_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{\\mu(A_k|S_k)}$$\n",
    "\n",
    "    **ordinary importance sampling**: $$V(s)=\\frac{\\sum_{t\\in \\mathcal{T}(s)}\\rho_t^{T(t)}G_t}{|\\mathcal{T}(s)|}$$ \n",
    "    \n",
    "    **weighted importance sampling**: $$V(s)=\\frac{\\sum_{t\\in \\mathcal{T}(s)}\\rho_t^{T(t)}G_t}{\\sum_{t\\in \\mathcal{T}(s)}\\rho_t^{T(t)}}$$\n",
    "    \n",
    "    where we keep numbering time across episodes (if first ends at time 100 then next begins at time 101), $\\mathcal{T}(s)$ is set of all time steps in which state $s$ was visited and $T(t)$ is the number of final step in episode in which time $t$ occured.\n",
    "* in the final algorithm we estimate state action values and based on algorithm from page 119 this is the estimate (with weighted importance sampling):\n",
    "    $$Q(s,a)=\\frac{\\sum_{t\\in \\mathcal{T}(s)}\\rho_{t+1}^{T(t+1)}G_t}{\\sum_{t\\in \\mathcal{T}(s)}\\rho_{t+1}^{T(t+1)}}$$\n",
    "    \n",
    "    I would compute it like this:\n",
    "    $$Q(s,a)=\\frac{\\sum_{t\\in \\mathcal{T}(s)}R_t + \\gamma V(S_{t+1})}{|\\mathcal{T}(s)|}$$\n",
    "    \n",
    "    where $V(S_{t+1})$ is the weighted importance sampling version of value estimation given above. Hmm, **the difference is only in the way to compute expected rewards from taking action $a$ in state $s$**. In first case we have weighted average and in the other sample average.\n",
    "    \n",
    "* Ordinary importance sampling is unbiased but it has potentially unlimited (even infinite) varinace. Weighted importance sampling is asymptotically unbiased and has finite limited varinace (if we have bounded returns then the variance converges to zero). In practice the latter is preferred.\n",
    "* In the perfect scenario we are able to do **exploring starts** which means we can start an episode in arbitrary state taking arbitrary action. Then for policy evaluation we compute sample returns from exploring starts. To find optimal policy we interleave policy evaluation and improvement (we can speed it up by doing policy improvements after each episode). This procedure is called **Monte Carlo ES**. **Caveat:** it has not been formally proven that this procedure converges to optimal policy.\n",
    "\n",
    "Questions:\n",
    "* the state-action values we are learning are non-stationary, because we do value and policy iteration. (It becomes stationary at some point) Then why not use weighted averages instead of sample averages (like in bandits for non-stationary problems). Hmm, if it becomes stationary then it doesn't matter in the limit.\n",
    "* how to explore smartly? Using the info we have so far (kind of using UCB)? Maybe limiting amount of exploration moves within a path?\n",
    "* maybe some way to do approx of exploring starts?\n",
    "* how to improve on long paths? (For episodic tasks. Unless you are punished for long episodes.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
